---

title: "Collaborative Filtering with Gradient Descent"
author: "Manoj Khara"
institute: "Indian Institute of Technology, Madras"
format:
html:
toc: true
code-fold: true
css: styles.css
---------------

## 🎯 Objective

Learn how to implement collaborative filtering using **gradient descent**, with side-by-side **mathematical equations** and **Python code**.

---

## 📘 Problem Setup

We have:

* \$Y \in \mathbb{R}^{n\_{\text{items}} \times n\_{\text{users}}}\$: rating matrix
* \$R \in {0, 1}^{n\_{\text{items}} \times n\_{\text{users}}}\$: indicator matrix
* \$X \in \mathbb{R}^{n\_{\text{items}} \times n\_{\text{features}}}\$: item features
* \$\Theta \in \mathbb{R}^{n\_{\text{users}} \times n\_{\text{features}}}\$: user features

---

## 🔢 Prediction Function

\::: {.columns}
\::: {.column width="50%"}

### **Math**

$$
\hat{Y} = X \Theta^T
$$

\:::

\::: {.column width="50%"}

### **Code**

```python
predictions = X @ Theta.T
```

\:::
\:::

---

## ❌ Error on Rated Entries

\::: {.columns}
\::: {.column width="50%"}

### **Math**

$$
E = (X \Theta^T - Y) \odot R
$$

\:::

\::: {.column width="50%"}

### **Code**

```python
error = (predictions - Y) * R
```

\:::
\:::

---

## 📉 Cost Function (with Regularization)

\::: {.columns}
\::: {.column width="50%"}

### **Math**

$$
J = \frac{1}{2} \sum_{i,j} R_{ij}(X_i \cdot \Theta_j^T - Y_{ij})^2 +
    \frac{\lambda}{2} \sum_i \|X_i\|^2 + \frac{\lambda}{2} \sum_j \|\Theta_j\|^2
$$

\:::

\::: {.column width="50%"}

### **Code**

```python
cost = 0.5 * np.sum(error ** 2)
cost += (lambda_r / 2) * (np.sum(X ** 2) + np.sum(Theta ** 2))
```

\:::
\:::

---

## 📐 Gradients (Vectorized)

\::: {.columns}
\::: {.column width="50%"}

### **Math**

$$
\frac{\partial J}{\partial X} = E \Theta + \lambda X \\
\frac{\partial J}{\partial \Theta} = E^T X + \lambda \Theta
$$

\:::

\::: {.column width="50%"}

### **Code**

```python
X_grad = error @ Theta + lambda_r * X
Theta_grad = error.T @ X + lambda_r * Theta
```

\:::
\:::

---

## 🔁 Parameter Update (Gradient Descent)

\::: {.columns}
\::: {.column width="50%"}

### **Math**

$$
X := X - \alpha \frac{\partial J}{\partial X} \\
\Theta := \Theta - \alpha \frac{\partial J}{\partial \Theta}
$$

\:::

\::: {.column width="50%"}

### **Code**

```python
X -= lr * X_grad
Theta -= lr * Theta_grad
```

\:::
\:::

---

## 🧠 Complete Training Function

```python
import numpy as np

def train_collaborative_filtering(Y, R, num_users, num_items, num_features,
                                   lr=0.005, lambda_r=0.1, epochs=100):
    X = np.random.randn(num_items, num_features)
    Theta = np.random.randn(num_users, num_features)
    losses = []

    for epoch in range(epochs):
        predictions = X @ Theta.T
        error = (predictions - Y) * R

        cost = 0.5 * np.sum(error ** 2)
        cost += (lambda_r / 2) * (np.sum(X ** 2) + np.sum(Theta ** 2))
        losses.append(cost)

        X_grad = error @ Theta + lambda_r * X
        Theta_grad = error.T @ X + lambda_r * Theta

        X -= lr * X_grad
        Theta -= lr * Theta_grad

        if epoch % 10 == 0 or epoch == epochs - 1:
            print(f"Epoch {epoch:03d}: Cost = {cost:.4f}")

    return X, Theta, losses
```

---

## 📊 Plotting the Loss Curve

```python
import matplotlib.pyplot as plt

plt.plot(losses)
plt.xlabel("Epochs")
plt.ylabel("Cost")
plt.title("Training Loss")
plt.grid(True)
plt.show()
```
